•	Payload- applications implement the map and reduce functions and form core of job.
•	Mapper- maps the input key to a set of intermediate keys.
•	Namenode- it manages hdfs
•	Datanode- where data is present in advance before any processing takes place.
•	Masternode- node where jobtracker runs and accept job requests from clients
•	Slavenode- where map and reduce program runs
•	Jobtracker- track task and track the assign jobs to tasktracker
# Namenode maintains a metanode in 2 files: fsimage and editlogs where fsimage initially loaded when hadoop system is started and it contains directory structure of clusters and data stored.then afterwards for every occuring transaction  edit log is updated.
# periodically per default every hour fsimage update.
question: why editlog will update on disk with new transaction and fsimage will update in main memory?
question:how does core of job form and what does it means?

